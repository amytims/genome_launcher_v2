// sanger-tol job config is here:
// https://github.com/sanger-tol/genomeassembly/blob/dev/conf/base.config

profiles {
    spartan {
        // Spartan limits. There is a bigmem queue with 3 TB available
        params {
            max_cpus = 72
            max_memory = 710000.MB
            max_time = 90.d
        }

        //  Note, it's tempting to use the apptainer profile, but the nf-core
        //  (and some sanger-tol) pipelines have a conditional
        //  `workflow.containerEngine == 'singularity'` that prevents using the
        //  right URL with apptainer.
        singularity {
            enabled = true
            autoMounts = true
            runOptions = '-B $PWD,$TMPDIR,/data --nv -H $(mktemp -d) --pwd $PWD --containall --cleanenv --writable-tmpfs'
        }

        // Submit up to 256 concurrent jobs (Setonix work partition max)
        executor {
            queueSize = 128
        }

        // Define process resource limits
        process {
            resourceLimits = [
                memory: 710000.MB,
                cpus: 72,
            ]
            executor = 'slurm'
            module = 'Apptainer/1.3.3'
            cache = 'lenient'
            stageInMode = 'symlink'
            queue = { task.memory > 710000.MB ? 'bigmem' : null }
        }
    }

    pawsey {
        workDir = "/scratch/pawsey1132/atims/genome_launcher_testing/work/"

        params {
            max_cpus = 256
            max_memory = 1020.GB
            max_time = 4.d
        }

        //  Note, it's tempting to use the apptainer profile, but the nf-core
        //  (and some sanger-tol) pipelines have a conditional
        //  `workflow.containerEngine == 'singularity'` that prevents using the
        //  right URL with apptainer.
        singularity {
            enabled = true
            autoMounts = true
            runOptions = '-B $PWD,$TMPDIR,/scratch -H $(mktemp -d) --pwd $PWD --containall --cleanenv --writable-tmpfs'
        }

        // Submit up to 256 concurrent jobs (Setonix work partition max)
        executor {
            queueSize = 128
        }

        // Define process resource limits
        process {
            resourceLimits = [
                memory: 1020.GB,
                cpus: 256,
            ]
            executor = 'slurm'
            module = 'singularity/4.1.0-nohost'
            cache = 'lenient'
            stageInMode = 'symlink'
            queue = { task.memory > 235520.MB ? 'highmem' : (task.time > 1.d ? 'long' : null) }
            // Try to avoid the long queue by redefining the time for jobs that
            // request more than 1.d on the first attempt. Subsequent attempts
            // won't be modified.

            // Pawsey is giving me error 125 when the OOM killer is active. Try
            // to override the default spec (which is here:
            // https://github.com/sanger-tol/genomeassembly/blob/31b508a3bd8998a27f6d06d5dc41bea4707b4a03/conf/base.config#L18)
            errorStrategy = { task.exitStatus in ((130..145) + 104 + 125) ? 'retry' : 'finish' }

            // Reduce the time for HIFIASM to avoid the long queue
            withName: '.*:HIFIASM.*' {
                time = { task.attempt == 1 ? 6.h : null }
                memory = { task.attempt == 1 ? 128.GB : null }
            }

            // BUSCO is taking a lot of memory so grab a whole node.
            withName: '.*BUSCO.*' {
                time = { task.attempt == 1 ? 1.d : null }
                memory = { task.attempt == 1 ? 128.GB : null }
                container = 'https://depot.galaxyproject.org/singularity/busco:5.8.2--pyhdfd78af_0'
            }

            withName: '.*MERQURYFK.*' {
                memory = { task.attempt == 1 ? 72.Gb : null }
            }

            withName: "*.SAMTOOLS_MERGE_HIC_MAPPING.*" {
                cpus = 8
                memory = 14.GB
            }
        }
        aws {
            client {
                endpoint = 'https://projects.pawsey.org.au'
                s3PathStyleAccess = true
                maxConnections = 4
                maxErrorRetry = 20
                uploadMaxAttempts = 20
            }
        }
    }
}